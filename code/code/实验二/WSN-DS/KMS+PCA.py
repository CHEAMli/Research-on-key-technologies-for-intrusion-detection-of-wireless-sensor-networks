import os
import glob
import gc
from pathlib import Path
from datetime import datetime
from collections import Counter

import numpy as np
import pandas as pd
import pyarrow.parquet as pq
from sklearn.cluster import KMeans  # æ–°å¢KMeanså¯¼å…¥
from sklearn.decomposition import PCA  # æ–°å¢PCAå¯¼å…¥

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    confusion_matrix, f1_score, precision_recall_fscore_support,
    precision_score, recall_score, roc_auc_score
)
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# ====================== åŸºæœ¬é…ç½® ======================
config = {
    # æ–°å¢KMSå’ŒPCAç›¸å…³é…ç½®
    "n_clusters": 10,  # KMeansèšç±»æ•°é‡
    "pca_components": 0.95,  # PCAä¿ç•™çš„æ–¹å·®æ¯”ä¾‹ï¼Œä¹Ÿå¯ä»¥æ˜¯æ•´æ•°ï¼ˆç»„ä»¶æ•°é‡ï¼‰
    "epochs": 100,
    "device": "cuda",
    "patience": 10,
    "num_classes": 5,
    "maj_threshold": 0.55,
    "test_size": 0.2,
    "random_state": 42,

    # RFå‚æ•°
    "rf_n_estimators": 200,
    "rf_max_depth": 15,
    "rf_min_samples_split": 5,
    "rf_min_samples_leaf": 2,
    "rf_class_weight": "balanced",

    # XGBå‚æ•°
    "xgb_n_estimators": 200,
    "xgb_max_depth": 8,
    "xgb_learning_rate": 0.1,
    "xgb_subsample": 0.8,
    "xgb_colsample_bytree": 0.8,
    "xgb_gamma": 0.1,
    "xgb_reg_alpha": 0.1,
    "xgb_reg_lambda": 1.0,
    "xgb_scale_pos_weight": 1,

    # Unknownæ£€æµ‹é˜ˆå€¼
    "unknown_threshold": 0.5,
    "energy_temperature": 1.0,
    "normal_fpr": 0.15,
    "allow_known_reject": 0.15,
}


# ====================== å›ºå®šéšæœºç§å­ ======================
def set_seed(seed: int = 42):
    import random
    random.seed(seed)
    np.random.seed(seed)


set_seed(config["random_state"])


# ====================== è¯»å– parquet æ–‡ä»¶å¤¹ ======================
def load_parquet_folder(folder_path: str) -> pd.DataFrame:
    parquet_files = [
        f for f in glob.glob(os.path.join(folder_path, "*"))
        if f.endswith(".parquet") and not f.endswith(".parquet.crc")
    ]
    print(f"ğŸ“‚ åœ¨ {folder_path} ä¸­å‘ç° {len(parquet_files)} ä¸ªæœ‰æ•ˆ Parquet æ–‡ä»¶")
    dfs = []
    for file in parquet_files:
        try:
            table = pq.read_table(file)
            df = table.to_pandas()
            dfs.append(df)
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡ {file}: {e}")
    if not dfs:
        raise RuntimeError(f"{folder_path} ä¸‹æ²¡æœ‰åˆæ³• parquet æ•°æ®")
    merged = pd.concat(dfs, ignore_index=True)
    return merged


# ====================== æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼ˆæ–°å¢KMS+PCAï¼‰ ======================
def preprocess_with_kms_pca(X_train, X_val, X_test, config):
    """ä½¿ç”¨KMSå’ŒPCAå¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†"""
    # 1. KMeansèšç±» - ä¸ºç‰¹å¾æ·»åŠ èšç±»ä¿¡æ¯
    print(f"ğŸ” æ‰§è¡ŒKMeansèšç±» (n_clusters={config['n_clusters']})...")
    kms = KMeans(n_clusters=config["n_clusters"], random_state=config["random_state"], n_init='auto')
    train_clusters = kms.fit_predict(X_train)
    val_clusters = kms.predict(X_val)
    test_clusters = kms.predict(X_test)

    # å°†èšç±»ç»“æœä½œä¸ºæ–°ç‰¹å¾æ·»åŠ 
    X_train_with_cluster = np.hstack([X_train, train_clusters.reshape(-1, 1)])
    X_val_with_cluster = np.hstack([X_val, val_clusters.reshape(-1, 1)])
    X_test_with_cluster = np.hstack([X_test, test_clusters.reshape(-1, 1)])

    # 2. PCAé™ç»´ - å‡å°‘ç‰¹å¾ç»´åº¦
    print(f"ğŸ” æ‰§è¡ŒPCAé™ç»´ (ä¿ç•™æ–¹å·®={config['pca_components']})...")
    pca = PCA(n_components=config["pca_components"], random_state=config["random_state"])
    X_train_pca = pca.fit_transform(X_train_with_cluster)
    X_val_pca = pca.transform(X_val_with_cluster)
    X_test_pca = pca.transform(X_test_with_cluster)

    print(f"ğŸ“Š PCAé™ç»´å®Œæˆ: åŸå§‹ç»´åº¦={X_train_with_cluster.shape[1]}, "
          f"é™ç»´åç»´åº¦={X_train_pca.shape[1]}")

    return X_train_pca, X_val_pca, X_test_pca, kms, pca


# ====================== æ•°æ®å‡†å¤‡å‡½æ•° ======================
def prepare_data(df: pd.DataFrame, label_col: str, feature_cols: list,
                 full2known: dict = None, is_test: bool = False):
    """å‡†å¤‡æ•°æ®ï¼Œç§»é™¤çª—å£æœºåˆ¶ï¼Œç›´æ¥ä½¿ç”¨æ¯ä¸ªæ—¶é—´ç‚¹çš„æ•°æ®"""
    X = df[feature_cols].values.astype(np.float32)

    if is_test:
        # æµ‹è¯•é›†ä¿ç•™åŸå§‹æ ‡ç­¾ç”¨äºè¯„ä¼°
        y_full = df[label_col].values.astype(int)
        if full2known:
            # å°†å·²çŸ¥ç±»æ˜ å°„ï¼ŒUnknownä¿æŒåŸçŠ¶
            y_known = np.array([full2known.get(label, -1) for label in y_full])
        else:
            y_known = y_full.copy()
        return X, y_known, y_full
    else:
        # è®­ç»ƒé›†ä½¿ç”¨æ˜ å°„åçš„æ ‡ç­¾
        y = df[label_col].values.astype(int)
        if full2known:
            y = np.array([full2known[label] for label in y])
        return X, y


# ====================== èƒ½é‡åˆ†æ•°è®¡ç®— ======================
def energy_score(probs: np.ndarray, T: float = 1.0) -> np.ndarray:
    """Energy score: E(x) = -T * logsumexp(logits/T). Higher => more OOD/unknown."""
    logits = np.log(probs + 1e-12)
    x = logits / T
    m = np.max(x, axis=1, keepdims=True)
    lse = m.squeeze(1) + np.log(np.sum(np.exp(x - m), axis=1) + 1e-12)
    return -T * lse


# ====================== æ¨¡å‹è®­ç»ƒå‡½æ•° ======================
def train_model(model, X_train, y_train, X_val, y_val, model_name="Model"):
    """è®­ç»ƒå¹¶è¿”å›æ¨¡å‹å’Œç»“æœ"""
    model.fit(X_train, y_train)

    # éªŒè¯é›†è¯„ä¼°
    y_val_pred = model.predict(X_val)
    y_val_proba = model.predict_proba(X_val)
    val_f1 = f1_score(y_val, y_val_pred, average="macro", zero_division=0)

    return {
        "model": model,
        "val_pred": y_val_pred,
        "val_proba": y_val_proba,
        "val_f1": val_f1,
        "val_true": y_val
    }


# ====================== ç²¾ç®€æ¨¡å‹è¯„ä¼°å‡½æ•° ======================
def evaluate_model_simple(model_result, X_test, y_test_full, y_test_known,
                          full2known, known2full, UNKNOWN_FULL, model_name="Model"):
    """ç²¾ç®€ç‰ˆæ¨¡å‹è¯„ä¼°ï¼Œåªè¾“å‡ºå…³é”®æŒ‡æ ‡"""
    model = model_result["model"]

    # æµ‹è¯•é›†é¢„æµ‹
    test_proba = model.predict_proba(X_test)
    test_pred_known = model.predict(X_test)
    test_pred_full = np.array([known2full[int(k)] for k in test_pred_known], dtype=int)

    # æ­£å¸¸ç±»ID
    normal_known_id = int(full2known.get(0, 0))

    # è®¡ç®—å„é¡¹åˆ†æ•°
    pnormal_test = test_proba[:, normal_known_id]
    s_attack_test = 1.0 - pnormal_test
    E_test = energy_score(test_proba, T=config["energy_temperature"])

    # åœ¨éªŒè¯é›†ä¸Šç¡®å®šé˜ˆå€¼
    val_proba = model_result["val_proba"]
    y_val_true = model_result["val_true"]
    E_val = energy_score(val_proba, T=config["energy_temperature"])
    pnormal_val = val_proba[:, normal_known_id]
    s_attack_val = 1.0 - pnormal_val

    # ç¡®å®šé˜ˆå€¼
    mask_norm_val = (y_val_true == normal_known_id)
    tau_energy = float(np.quantile(E_val, 1.0 - config["allow_known_reject"]))
    tau_attack = float(np.quantile(s_attack_val[mask_norm_val], 1.0 - config["normal_fpr"]))
    tau_p0_safe = float(np.quantile(pnormal_val[mask_norm_val], config["normal_fpr"]))

    # åº”ç”¨Unknownæ£€æµ‹
    final_pred = test_pred_full.copy()
    mask_high_energy = (E_test > tau_energy)
    mask_attack_like = (s_attack_test > tau_attack) & (pnormal_test < tau_p0_safe)
    mask_convert_to_unknown = (mask_high_energy | mask_attack_like) & (test_pred_full == 0)
    final_pred[mask_convert_to_unknown] = UNKNOWN_FULL

    # ------------ è®¡ç®—å››ç»„å…³é”®æŒ‡æ ‡ ------------
    results = {}

    # 1. æ”»å‡»æ£€æµ‹æŒ‡æ ‡
    gt_is_attack = (y_test_full != 0)
    pred_is_attack = (final_pred != 0)

    p_attack = precision_score(gt_is_attack.astype(int), pred_is_attack.astype(int), zero_division=0)
    r_attack = recall_score(gt_is_attack.astype(int), pred_is_attack.astype(int), zero_division=0)
    f1_attack = f1_score(gt_is_attack.astype(int), pred_is_attack.astype(int), zero_division=0)
    auc_attack = roc_auc_score(gt_is_attack.astype(int), s_attack_test)

    results["attack"] = {
        "precision": p_attack,
        "recall": r_attack,
        "f1": f1_attack,
        "auc": auc_attack
    }

    # 2. Unknownæ£€æµ‹æŒ‡æ ‡
    gt_is_unknown = (y_test_full == UNKNOWN_FULL)
    pred_is_unknown = (final_pred == UNKNOWN_FULL)

    if gt_is_unknown.sum() > 0:
        p_u = precision_score(gt_is_unknown.astype(int), pred_is_unknown.astype(int), zero_division=0)
        r_u = recall_score(gt_is_unknown.astype(int), pred_is_unknown.astype(int), zero_division=0)
        f1_u = f1_score(gt_is_unknown.astype(int), pred_is_unknown.astype(int), zero_division=0)
        auc_u = roc_auc_score(gt_is_unknown.astype(int), E_test)

        results["unknown"] = {
            "precision": p_u,
            "recall": r_u,
            "f1": f1_u,
            "auc": auc_u
        }
    else:
        results["unknown"] = None

    # 3. å·²çŸ¥ç±»åˆ†ç±»æŒ‡æ ‡
    mask_known_eval = (~gt_is_unknown) & (~pred_is_unknown)

    if mask_known_eval.sum() > 0:
        yk = np.array([full2known.get(int(y), -1) for y in y_test_full[mask_known_eval]])
        pk = np.array([full2known.get(int(y), -1) for y in final_pred[mask_known_eval]])

        valid_mask = (yk != -1) & (pk != -1)
        yk_valid = yk[valid_mask]
        pk_valid = pk[valid_mask]

        if len(yk_valid) > 0:
            macro_f1_known = f1_score(yk_valid, pk_valid, average="macro", zero_division=0)
            coverage = float(mask_known_eval.mean())

            results["known"] = {
                "macro_f1": macro_f1_known,
                "coverage": coverage
            }
        else:
            results["known"] = None
    else:
        results["known"] = None

    # 4. Grayhole breakdownæŒ‡æ ‡
    mask_gh = (y_test_full == UNKNOWN_FULL)
    n_gh = int(mask_gh.sum())

    if n_gh > 0:
        gh_as_unknown = int((mask_gh & pred_is_unknown).sum())
        gh_as_normal = int((mask_gh & (final_pred == 0)).sum())

        # è®¡ç®—è¢«é¢„æµ‹ä¸ºå…¶ä»–æ”»å‡»çš„Grayhole
        other_attack_labels = [label for label in known2full.values()
                               if label != 0 and label != UNKNOWN_FULL]
        gh_as_other = 0
        other_attack_details = {}

        for attack_label in other_attack_labels:
            count = int((mask_gh & (final_pred == attack_label)).sum())
            gh_as_other += count
            if count > 0:
                other_attack_details[attack_label] = count

        results["grayhole"] = {
            "total": n_gh,
            "unknown": gh_as_unknown,
            "normal": gh_as_normal,
            "other_attack": gh_as_other,
            "other_details": other_attack_details
        }
    else:
        results["grayhole"] = None

    return results


# ====================== ä¸»è®­ç»ƒæµç¨‹ ======================
def main():
    save_dir = Path("saved_models_tree_simple")
    save_dir.mkdir(exist_ok=True)
    start_time = datetime.now()
    run_tag = start_time.strftime("%Y%m%d-%H%M%S")

    # ------------ 1. åŠ è½½åŸå§‹è®­ç»ƒ/æµ‹è¯•æ•°æ® ------------
    print("ğŸš€ å¼€å§‹åŠ è½½ WSN-DS æ•°æ®é›†...")
    train_df = load_parquet_folder(r"D:\workspace\TTS-XGB\data\dataset_processing\newtrain.parquet")
    test_df = load_parquet_folder(r"D:\workspace\TTS-XGB\data\dataset_processing\test.parquet")

    ID_COL = "id"
    TIME_COL = "Time"
    LABEL_COL = "Attack_type"

    UNKNOWN_FULL = 2  # Grayhole åœ¨ä½ çš„æ•°æ®é‡Œå°±æ˜¯ 2

    feature_cols = [c for c in train_df.columns if c not in [ID_COL, TIME_COL, LABEL_COL]]

    # ------------ 2. æ ‡å‡†åŒ– + å¡«å……ç¼ºå¤± ------------
    scaler = StandardScaler()
    train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])
    test_df[feature_cols] = scaler.transform(test_df[feature_cols])

    train_df = train_df.ffill().bfill()
    test_df = test_df.ffill().bfill()

    # ------------ 3. å·²çŸ¥ç±»æ˜ å°„ ------------
    known_full = sorted([int(x) for x in train_df[LABEL_COL].unique().tolist()])
    assert UNKNOWN_FULL not in known_full, f"newtrain é‡Œå±…ç„¶è¿˜æœ‰ Unknown={UNKNOWN_FULL}ï¼Œå…ˆæ£€æŸ¥æ•°æ®é›†ï¼"

    full2known = {full: i for i, full in enumerate(known_full)}
    known2full = {i: full for full, i in full2known.items()}

    K = len(known_full)
    config["num_classes"] = K

    # ------------ 4. å‡†å¤‡æ•°æ® ------------
    # è®­ç»ƒæ•°æ®
    X_train_all, y_train_all = prepare_data(train_df, LABEL_COL, feature_cols, full2known)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_all, y_train_all,
        test_size=config["test_size"],
        random_state=config["random_state"],
        stratify=y_train_all
    )

    # æµ‹è¯•æ•°æ®
    X_test, y_test_known, y_test_full = prepare_data(
        test_df, LABEL_COL, feature_cols, full2known, is_test=True
    )

    # æ–°å¢ï¼šåº”ç”¨KMS+PCAé¢„å¤„ç†
    X_train, X_val, X_test, kms_model, pca_model = preprocess_with_kms_pca(
        X_train, X_val, X_test, config
    )

    # ------------ 5. è®­ç»ƒæ‰€æœ‰æ¨¡å‹ ------------
    print(f"\n{'=' * 60}")
    print("è®­ç»ƒæ¨¡å‹...")
    print(f"{'=' * 60}")

    all_results = {}

    # 1. è®­ç»ƒéšæœºæ£®æ—
    print("\nğŸŒ² è®­ç»ƒéšæœºæ£®æ—...")
    rf_model = RandomForestClassifier(
        n_estimators=config["rf_n_estimators"],
        max_depth=config["rf_max_depth"],
        min_samples_split=config["rf_min_samples_split"],
        min_samples_leaf=config["rf_min_samples_leaf"],
        class_weight=config["rf_class_weight"],
        random_state=config["random_state"],
        n_jobs=-1
    )

    rf_result = train_model(rf_model, X_train, y_train, X_val, y_val, "Random Forest")

    # 2. è®­ç»ƒXGBoost
    print("ğŸŒ³ è®­ç»ƒXGBoost...")
    xgb_model = XGBClassifier(
        n_estimators=config["xgb_n_estimators"],
        max_depth=config["xgb_max_depth"],
        learning_rate=config["xgb_learning_rate"],
        subsample=config["xgb_subsample"],
        colsample_bytree=config["xgb_colsample_bytree"],
        gamma=config["xgb_gamma"],
        reg_alpha=config["xgb_reg_alpha"],
        reg_lambda=config["xgb_reg_lambda"],
        scale_pos_weight=config["xgb_scale_pos_weight"],
        random_state=config["random_state"],
        use_label_encoder=False,
        eval_metric='mlogloss',
        device=config["device"],
        tree_method='gpu_hist' if config["device"] == "cuda" else 'auto'
    )

    xgb_result = train_model(xgb_model, X_train, y_train, X_val, y_val, "XGBoost")

    # ------------ 6. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ‰€æœ‰æ¨¡å‹ ------------
    print(f"\n{'=' * 60}")
    print("è¯„ä¼°ç»“æœ")
    print(f"{'=' * 60}")

    # è¯„ä¼°éšæœºæ£®æ—
    rf_eval = evaluate_model_simple(
        rf_result, X_test, y_test_full, y_test_known,
        full2known, known2full, UNKNOWN_FULL, "Random Forest"
    )
    all_results["rf"] = rf_eval

    # è¯„ä¼°XGBoost
    xgb_eval = evaluate_model_simple(
        xgb_result, X_test, y_test_full, y_test_known,
        full2known, known2full, UNKNOWN_FULL, "XGBoost"
    )
    all_results["xgb"] = xgb_eval

    # ------------ 7. è¾“å‡ºå››ç»„å…³é”®æŒ‡æ ‡ ------------
    print(f"\n{'=' * 60}")
    print("ğŸ“Š å››ç»„å…³é”®æŒ‡æ ‡å¯¹æ¯”")
    print(f"{'=' * 60}")

    # 1. æ”»å‡»æ£€æµ‹æŒ‡æ ‡
    print("\nğŸ” 1. æ”»å‡»æ£€æµ‹æ€§èƒ½:")
    print(f"{'æ¨¡å‹':<12} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'AUROC':<8}")
    for model_name, results in all_results.items():
        if results["attack"]:
            attack = results["attack"]
            print(f"{model_name:<12} {attack['precision']:<8.4f} {attack['recall']:<8.4f} "
                  f"{attack['f1']:<8.4f} {attack['auc']:<8.4f}")

    # 2. Unknownæ£€æµ‹æŒ‡æ ‡
    print("\nğŸ” 2. Unknownæ£€æµ‹æ€§èƒ½:")
    print(f"{'æ¨¡å‹':<12} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'AUROC':<8}")
    for model_name, results in all_results.items():
        if results["unknown"]:
            unknown = results["unknown"]
            print(f"{model_name:<12} {unknown['precision']:<8.4f} {unknown['recall']:<8.4f} "
                  f"{unknown['f1']:<8.4f} {unknown['auc']:<8.4f}")
        else:
            print(f"{model_name:<12} {'N/A':<8} {'N/A':<8} {'N/A':<8} {'N/A':<8}")

    # 3. å·²çŸ¥ç±»åˆ†ç±»æŒ‡æ ‡
    print("\nğŸ” 3. å·²çŸ¥ç±»åˆ†ç±»æ€§èƒ½:")
    print(f"{'æ¨¡å‹':<12} {'Macro-F1':<8} {'è¦†ç›–ç‡':<8}")
    for model_name, results in all_results.items():
        if results["known"]:
            known = results["known"]
            print(f"{model_name:<12} {known['macro_f1']:<8.4f} {known['coverage']:<8.4f}")
        else:
            print(f"{model_name:<12} {'N/A':<8} {'N/A':<8}")

    # 4. Grayhole breakdownæŒ‡æ ‡
    print("\nğŸ” 4. Grayholeå»å‘åˆ†æ:")
    print(f"{'æ¨¡å‹':<12} {'æ€»æ•°':<8} {'Unknown':<8} {'Normal':<8} {'å…¶ä»–æ”»å‡»':<8}")
    for model_name, results in all_results.items():
        if results["grayhole"]:
            grayhole = results["grayhole"]
            print(f"{model_name:<12} {grayhole['total']:<8} {grayhole['unknown']:<8} "
                  f"{grayhole['normal']:<8} {grayhole['other_attack']:<8}")

            # å¦‚æœæœ‰å…¶ä»–æ”»å‡»è¯¦ç»†åˆ†å¸ƒï¼Œæ˜¾ç¤ºåœ¨ä¸‹ä¸€è¡Œ
            if grayhole["other_details"]:
                print(f"  è¯¦ç»†åˆ†å¸ƒ: {grayhole['other_details']}")
        else:
            print(f"{model_name:<12} {'N/A':<8} {'N/A':<8} {'N/A':<8} {'N/A':<8}")

    # ------------ 8. ä¿å­˜ç»“æœ ------------
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœ...")
    import joblib

    # ä¿å­˜æ¨¡å‹
    rf_model_path = save_dir / "rf_model.joblib"
    joblib.dump(rf_model, rf_model_path)

    xgb_model_path = save_dir / "xgb_model.joblib"
    joblib.dump(xgb_model, xgb_model_path)

    # ä¿å­˜é¢„å¤„ç†ç›¸å…³æ¨¡å‹ï¼ˆæ–°å¢ï¼‰
    kms_path = save_dir / "kms_model.joblib"
    joblib.dump(kms_model, kms_path)

    pca_path = save_dir / "pca_model.joblib"
    joblib.dump(pca_model, pca_path)

    # ä¿å­˜scaler
    scaler_path = save_dir / "scaler.joblib"
    joblib.dump(scaler, scaler_path)

    # ä¿å­˜å®Œæ•´ç»“æœ
    results_path = save_dir / "results_summary.txt"
    with open(results_path, "w", encoding="utf-8") as f:
        f.write(f"å®éªŒæ—¶é—´: {run_tag}\n")
        f.write(f"å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"å·²çŸ¥ç±»æ•°é‡: {K}\n")
        f.write(f"Unknownæ ‡ç­¾: {UNKNOWN_FULL}\n")
        f.write(f"KMSèšç±»æ•°é‡: {config['n_clusters']}\n")  # æ–°å¢
        f.write(f"PCAé™ç»´åç»´åº¦: {X_train.shape[1]}\n\n")  # æ–°å¢

        f.write("1. æ”»å‡»æ£€æµ‹æ€§èƒ½:\n")
        f.write(f"{'æ¨¡å‹':<12} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'AUROC':<8}\n")
        for model_name, results in all_results.items():
            if results["attack"]:
                attack = results["attack"]
                f.write(f"{model_name:<12} {attack['precision']:<8.4f} {attack['recall']:<8.4f} "
                        f"{attack['f1']:<8.4f} {attack['auc']:<8.4f}\n")

        f.write("\n2. Unknownæ£€æµ‹æ€§èƒ½:\n")
        f.write(f"{'æ¨¡å‹':<12} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'AUROC':<8}\n")
        for model_name, results in all_results.items():
            if results["unknown"]:
                unknown = results["unknown"]
                f.write(f"{model_name:<12} {unknown['precision']:<8.4f} {unknown['recall']:<8.4f} "
                        f"{unknown['f1']:<8.4f} {unknown['auc']:<8.4f}\n")
            else:
                f.write(f"{model_name:<12} {'N/A':<8} {'N/A':<8} {'N/A':<8} {'N/A':<8}\n")

        f.write("\n3. å·²çŸ¥ç±»åˆ†ç±»æ€§èƒ½:\n")
        f.write(f"{'æ¨¡å‹':<12} {'Macro-F1':<8} {'è¦†ç›–ç‡':<8}\n")
        for model_name, results in all_results.items():
            if results["known"]:
                known = results["known"]
                f.write(f"{model_name:<12} {known['macro_f1']:<8.4f} {known['coverage']:<8.4f}\n")
            else:
                f.write(f"{model_name:<12} {'N/A':<8} {'N/A':<8}\n")

        f.write("\n4. Grayholeå»å‘åˆ†æ:\n")
        f.write(f"{'æ¨¡å‹':<12} {'æ€»æ•°':<8} {'Unknown':<8} {'Normal':<8} {'å…¶ä»–æ”»å‡»':<8}\n")
        for model_name, results in all_results.items():
            if results["grayhole"]:
                grayhole = results["grayhole"]
                f.write(f"{model_name:<12} {grayhole['total']:<8} {grayhole['unknown']:<8} "
                        f"{grayhole['normal']:<8} {grayhole['other_attack']:<8}\n")

                if grayhole["other_details"]:
                    f.write(f"  è¯¦ç»†åˆ†å¸ƒ: {grayhole['other_details']}\n")
            else:
                f.write(f"{model_name:<12} {'N/A':<8} {'N/A':<8} {'N/A':<8} {'N/A':<8}\n")

    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
    end_time = datetime.now()
    duration = end_time - start_time
    print(f"â±ï¸  æ€»è€—æ—¶: {duration}")


if __name__ == "__main__":
    main()