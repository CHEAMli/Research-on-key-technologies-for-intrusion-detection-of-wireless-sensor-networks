import os
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import (
    classification_report, confusion_matrix, precision_recall_fscore_support,
    f1_score, precision_score, recall_score, roc_auc_score
)
from sklearn.preprocessing import StandardScaler
from typing import Tuple, Dict, List

# è·¯å¾„é…ç½®
TRAIN_FILE = "D:\\workspace\\TTS-XGB\\data\\new TON-IoT\\train_no_injection_with_origidx.csv"
TEST_FILE = "D:\\workspace\\TTS-XGB\\data\\new TON-IoT\\test_full_with_injection_with_origidx.csv"
RESULTS_DIR = "D:\\workspace\\TTS-XGB\\results"
os.makedirs(RESULTS_DIR, exist_ok=True)


def create_global_label_mapping(train_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple[Dict[str, int], List[str]]:
    """åˆ›å»ºå…¨å±€æ ‡ç­¾æ˜ å°„ï¼ˆè§£å†³è®­ç»ƒé›†ç±»åˆ«ä¸å…¨é—®é¢˜ï¼‰"""
    # 1. æ”¶é›†è®­ç»ƒ+æµ‹è¯•é›†çš„æ‰€æœ‰å”¯ä¸€æ ‡ç­¾
    train_labels = train_df['type'].unique()
    test_labels = test_df['type'].unique()
    all_labels = sorted(list(set(train_labels) | set(test_labels)))  # åˆå¹¶å¹¶æ’åº

    # 2. å»ºç«‹å…¨å±€è¿ç»­ç´¢å¼•æ˜ å°„ï¼ˆ0å¼€å§‹ï¼‰
    label_mapping = {label: idx for idx, label in enumerate(all_labels)}

    # æ‰“å°ç±»åˆ«åˆ†å¸ƒä¿¡æ¯
    print("\n===== ç±»åˆ«åˆ†å¸ƒè¯¦æƒ… =====")
    print(f"æ‰€æœ‰ç±»åˆ«ï¼ˆå…±{len(all_labels)}ä¸ªï¼‰: {all_labels}")
    print(f"è®­ç»ƒé›†åŒ…å«ç±»åˆ«: {sorted(train_labels)}")
    print(f"æµ‹è¯•é›†åŒ…å«ç±»åˆ«: {sorted(test_labels)}")
    print(f"è®­ç»ƒé›†ç¼ºå¤±çš„ç±»åˆ«: {[l for l in all_labels if l not in train_labels]}")
    print(f"å…¨å±€æ ‡ç­¾æ˜ å°„: {label_mapping}")

    return label_mapping, all_labels


def prepare_data() -> Tuple[
    Tuple[np.ndarray, np.ndarray, np.ndarray],
    Tuple[np.ndarray, np.ndarray, np.ndarray],
    Dict[str, int], List[str]
]:
    """æ•°æ®é¢„å¤„ç†ï¼ˆä»…ä¿ç•™æ ‡å‡†åŒ–ï¼‰"""
    print("Loading data...")
    train_df = pd.read_csv(TRAIN_FILE)
    test_df = pd.read_csv(TEST_FILE)

    print(f"Train shape: {train_df.shape}, Test shape: {test_df.shape}")

    # æ•°å€¼ç‰¹å¾åˆ—è¡¨
    numeric_features = ['src_port', 'dst_port', 'duration', 'src_bytes', 'dst_bytes',
                        'missed_bytes', 'src_pkts', 'src_ip_bytes', 'dst_pkts', 'dst_ip_bytes',
                        'dns_qclass', 'dns_qtype', 'dns_rcode',
                        'http_request_body_len', 'http_response_body_len', 'http_status_code']

    # åˆ›å»ºå…¨å±€æ ‡ç­¾æ˜ å°„
    label_mapping, all_labels = create_global_label_mapping(train_df, test_df)

    # è½¬æ¢æ ‡ç­¾ä¸ºè¿ç»­ç´¢å¼•
    y_train = np.array([label_mapping[cls] for cls in train_df['type'].values])
    y_test = np.array([label_mapping[cls] for cls in test_df['type'].values])

    # åŸå§‹ç±»å‹ä¿ç•™
    type_train = train_df['type'].values
    type_test = test_df['type'].values

    # ç‰¹å¾é¢„å¤„ç†ï¼ˆä»…ä¿ç•™æ ‡å‡†åŒ–ï¼‰
    X_train = train_df[numeric_features].fillna(0).values.astype(np.float32)
    X_test = test_df[numeric_features].fillna(0).values.astype(np.float32)

    # æ ‡å‡†åŒ–
    print("\nApplying StandardScaler...")
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    print(f"æ ‡å‡†åŒ–åç‰¹å¾ç»´åº¦: {X_train.shape[1]}")

    return (X_train, y_train, type_train), (X_test, y_test, type_test), label_mapping, all_labels


def train_xgb_native(X_train: np.ndarray, y_train: np.ndarray, num_classes: int,
                     n_estimators: int = 100, max_depth: int = 3) -> xgb.Booster:
    """ä½¿ç”¨XGBooståŸç”Ÿæ¥å£è®­ç»ƒæ¨¡å‹"""
    print("\nTraining XGBoost model (native API)...")

    # è½¬æ¢ä¸ºXGBoostçš„DMatrixæ ¼å¼
    dtrain = xgb.DMatrix(X_train, label=y_train)

    # è®¾ç½®XGBoostå‚æ•°
    params = {
        'objective': 'multi:softprob',
        'num_class': num_classes,
        'max_depth': max_depth,
        'eta': 0.1,
        'seed': 42,
        'silent': 1,
        'eval_metric': 'mlogloss'
    }

    # è®­ç»ƒæ¨¡å‹
    booster = xgb.train(
        params,
        dtrain,
        num_boost_round=n_estimators,
        verbose_eval=False
    )

    return booster


def predict_xgb_native(booster: xgb.Booster, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """ä½¿ç”¨åŸç”Ÿæ¥å£é¢„æµ‹ï¼šè¿”å›ç±»åˆ«ç´¢å¼• + æ¦‚ç‡çŸ©é˜µ"""
    dtest = xgb.DMatrix(X)
    prob_matrix = booster.predict(dtest)  # å½¢çŠ¶: (n_samples, num_classes)
    y_pred = np.argmax(prob_matrix, axis=1)  # å–æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«
    return y_pred, prob_matrix


def main():
    # è¶…å‚æ•°
    n_estimators = int(os.environ.get("N_ESTIMATORS", 100))
    max_depth = int(os.environ.get("MAX_DEPTH", 3))
    unknown_threshold = 0.6  # Unknownæ£€æµ‹é˜ˆå€¼

    # æ•°æ®é¢„å¤„ç†
    (X_train, y_train, type_train), (X_test, y_test, type_test), label_mapping, all_labels = prepare_data()
    num_classes = len(all_labels)
    print(f"\nç‰¹å¾ç»´åº¦: {X_train.shape[1]}, å…¨å±€ç±»åˆ«æ€»æ•°: {num_classes}")

    # è®­ç»ƒæ¨¡å‹
    booster = train_xgb_native(
        X_train,
        y_train,
        num_classes=num_classes,
        n_estimators=n_estimators,
        max_depth=max_depth
    )

    # é¢„æµ‹
    print("Predicting on test set...")
    y_pred, prob_matrix = predict_xgb_native(booster, X_test)
    max_probs = np.max(prob_matrix, axis=1)  # æ¯ä¸ªæ ·æœ¬çš„æœ€å¤§æ¦‚ç‡

    # ============== A) äºŒåˆ†ç±»è¯„ä¼° (normal vs å…¶ä»–) ==============
    print("\n===== A) äºŒåˆ†ç±»è¯„ä¼° (normal vs å…¶ä»–) =====")
    try:
        normal_idx = label_mapping['normal']
        # æ„å»ºäºŒåˆ†ç±»æ ‡ç­¾ï¼š0=normalï¼Œ1=å…¶ä»–
        y_test_binary = (y_test != normal_idx).astype(int)
        y_pred_binary = (y_pred != normal_idx).astype(int)

        # è®¡ç®—æŒ‡æ ‡
        precision = precision_score(y_test_binary, y_pred_binary, zero_division=0)
        recall = recall_score(y_test_binary, y_pred_binary, zero_division=0)
        f1 = f1_score(y_test_binary, y_pred_binary, zero_division=0)

        # è®¡ç®—AUROCï¼ˆä½¿ç”¨normalç±»çš„æ¦‚ç‡ä½œä¸ºè¯„åˆ†ï¼‰
        normal_probs = prob_matrix[:, normal_idx]
        attack_probs = 1 - normal_probs  # å…¶ä»–ç±»çš„æ¦‚ç‡
        auroc = roc_auc_score(y_test_binary, attack_probs)

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_test_binary, y_pred_binary)

        # è¾“å‡ºç»“æœ
        print(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
        print(f"å¬å›ç‡ (Recall): {recall:.4f}")
        print(f"F1åˆ†æ•°: {f1:.4f}")
        print(f"AUROC: {auroc:.4f}")
        print("æ··æ·†çŸ©é˜µ:")
        print(cm)
        print("è¡Œ: çœŸå®æ ‡ç­¾ (0=normal, 1=å…¶ä»–), åˆ—: é¢„æµ‹æ ‡ç­¾")

    except KeyError:
        print("âš ï¸  æœªæ‰¾åˆ°'normal'æ ‡ç­¾ï¼Œæ— æ³•è¿›è¡ŒäºŒåˆ†ç±»è¯„ä¼°")

    # ============== B) å·²çŸ¥æ”»å‡»å¤šåˆ†ç±»è¯„ä¼° (é™¤injection) ==============
    print("\n===== B) å·²çŸ¥æ”»å‡»å¤šåˆ†ç±»è¯„ä¼° (é™¤injection) =====")
    try:
        # ç¡®å®šå·²çŸ¥ç±»åˆ«ï¼ˆæ‰€æœ‰ç±»åˆ«é™¤äº†injectionï¼‰
        known_labels = [label for label in all_labels if label != 'injection']
        known_indices = [label_mapping[label] for label in known_labels]

        # ç­›é€‰å·²çŸ¥ç±»æ ·æœ¬ï¼ˆæµ‹è¯•é›†ä¸­éinjectionæ ·æœ¬ï¼‰
        mask_known = (type_test != 'injection')
        y_test_known = y_test[mask_known]
        y_pred_known = y_pred[mask_known]
        max_probs_known = max_probs[mask_known]

        if len(y_test_known) == 0:
            print("âš ï¸  æµ‹è¯•é›†ä¸­æ²¡æœ‰å·²çŸ¥ç±»æ ·æœ¬")
        else:
            # è®¡ç®—Macro-F1
            macro_f1_known = f1_score(
                y_test_known, y_pred_known,
                average='macro',
                labels=known_indices,
                zero_division=0
            )

            # è®¡ç®—è¦†ç›–ç‡ï¼ˆé¢„æµ‹ä¸ºå·²çŸ¥ç±»ä¸”æ¦‚ç‡â‰¥é˜ˆå€¼çš„æ ·æœ¬æ¯”ä¾‹ï¼‰
            mask_confident = (max_probs_known >= unknown_threshold)
            coverage_known = mask_confident.mean()

            print(f"å·²çŸ¥ç±»åˆ—è¡¨: {known_labels}")
            print(f"Macro-F1_known: {macro_f1_known:.4f}")
            print(f"coverage_known: {coverage_known:.4f} (é˜ˆå€¼={unknown_threshold})")

    except Exception as e:
        print(f"âš ï¸  å·²çŸ¥ç±»è¯„ä¼°å‡ºé”™: {str(e)}")

    # ============== C) Unknown(injection) åˆ†æ ==============
    print("\n===== C) Unknown(injection) åˆ†æ =====")
    try:
        # ç­›é€‰injectionæ ·æœ¬
        mask_injection = (type_test == 'injection')
        total_injection = int(mask_injection.sum())

        if total_injection == 0:
            print("âš ï¸  æµ‹è¯•é›†ä¸­æœªæ‰¾åˆ°injectionæ ·æœ¬")
        else:
            # è·å–injectionæ ·æœ¬çš„é¢„æµ‹ç»“æœ
            injection_pred = y_pred[mask_injection]
            injection_probs = prob_matrix[mask_injection]
            injection_maxp = max_probs[mask_injection]
            normal_idx = label_mapping['normal']

            # 1. å»å‘ç»Ÿè®¡
            # é¢„æµ‹ä¸ºNormal
            pred_normal = int(((injection_pred == normal_idx) & (injection_maxp >= unknown_threshold)).sum())
            # é¢„æµ‹ä¸ºUnknown
            pred_unknown = int((injection_maxp < unknown_threshold).sum())
            # é¢„æµ‹ä¸ºå…¶ä»–å·²çŸ¥æ”»å‡»
            pred_known_attack = 0
            known_attack_details = {}
            for label, idx in label_mapping.items():
                if label not in ['injection', 'normal']:
                    count = int(((injection_pred == idx) & (injection_maxp >= unknown_threshold)).sum())
                    pred_known_attack += count
                    if count > 0:
                        known_attack_details[label] = count

            # 2. Unknownæ£€æµ‹F1ï¼ˆè§†injectionä¸ºæ­£æ ·æœ¬ï¼Œå…¶ä»–ä¸ºè´Ÿæ ·æœ¬ï¼‰
            # æ„å»ºæ ‡ç­¾ï¼š1=injectionï¼Œ0=å…¶ä»–
            y_unknown_gt = mask_injection.astype(int)
            # æ„å»ºé¢„æµ‹ï¼š1=é¢„æµ‹ä¸ºUnknownï¼Œ0=å…¶ä»–
            y_unknown_pred = (max_probs < unknown_threshold).astype(int)

            u_precision = precision_score(y_unknown_gt, y_unknown_pred, zero_division=0)
            u_recall = recall_score(y_unknown_gt, y_unknown_pred, zero_division=0)
            u_f1 = f1_score(y_unknown_gt, y_unknown_pred, zero_division=0)

            # è¾“å‡ºç»“æœ
            print(f"Injectionæ ·æœ¬æ€»æ•°: {total_injection}")
            print(f"  â†’ Normal   : {pred_normal} ({pred_normal / total_injection * 100:.2f}%)")
            print(f"  â†’ Unknown  : {pred_unknown} ({pred_unknown / total_injection * 100:.2f}%)")
            print(f"  â†’ å…¶ä»–å·²çŸ¥æ”»å‡»: {pred_known_attack} ({pred_known_attack / total_injection * 100:.2f}%)")
            if known_attack_details:
                print(f"  è¯¦ç»†åˆ†å¸ƒ: {known_attack_details}")

            print(f"\nUnknownæ£€æµ‹F1: {u_f1:.4f}")
            print(f"Unknownæ£€æµ‹ç²¾ç¡®ç‡: {u_precision:.4f}")
            print(f"Unknownæ£€æµ‹å¬å›ç‡: {u_recall:.4f}")

    except KeyError as e:
        print(f"âš ï¸  æœªæ‰¾åˆ°æ ‡ç­¾: {e}")
        print(f"å½“å‰å…¨å±€æ ‡ç­¾åˆ—è¡¨: {all_labels}")

    # ä¿å­˜ç»“æœ
    print("\nğŸ“ ç»“æœæ–‡ä»¶ä¿å­˜è·¯å¾„:")
    report_path = os.path.join(RESULTS_DIR, "xgb_model_evaluation_report.txt")
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("===== æ¨¡å‹è¯„ä¼°æŠ¥å‘Š =====\n")
        f.write(f"æ—¶é—´: {pd.Timestamp.now()}\n")
        f.write(f"å‚æ•°: n_estimators={n_estimators}, max_depth={max_depth}, unknown_threshold={unknown_threshold}\n\n")

        # ä¿å­˜Aéƒ¨åˆ†
        f.write("===== A) äºŒåˆ†ç±»è¯„ä¼° (normal vs å…¶ä»–) =====\n")
        if 'normal_idx' in locals():
            f.write(f"ç²¾ç¡®ç‡: {precision:.4f}\n")
            f.write(f"å¬å›ç‡: {recall:.4f}\n")
            f.write(f"F1åˆ†æ•°: {f1:.4f}\n")
            f.write(f"AUROC: {auroc:.4f}\n")
            f.write("æ··æ·†çŸ©é˜µ:\n")
            f.write(f"{cm}\n\n")
        else:
            f.write("æœªæ‰¾åˆ°'normal'æ ‡ç­¾ï¼Œæ— æ³•è¯„ä¼°\n\n")

        # ä¿å­˜Béƒ¨åˆ†
        f.write("===== B) å·²çŸ¥æ”»å‡»å¤šåˆ†ç±»è¯„ä¼° =====\n")
        if 'macro_f1_known' in locals():
            f.write(f"å·²çŸ¥ç±»åˆ—è¡¨: {known_labels}\n")
            f.write(f"Macro-F1_known: {macro_f1_known:.4f}\n")
            f.write(f"coverage_known: {coverage_known:.4f}\n\n")
        else:
            f.write("å·²çŸ¥ç±»è¯„ä¼°å¤±è´¥\n\n")

        # ä¿å­˜Céƒ¨åˆ†
        f.write("===== C) Unknown(injection) åˆ†æ =====\n")
        if total_injection > 0:
            f.write(f"Injectionæ ·æœ¬æ€»æ•°: {total_injection}\n")
            f.write(f"â†’ Normal: {pred_normal} ({pred_normal / total_injection * 100:.2f}%)\n")
            f.write(f"â†’ Unknown: {pred_unknown} ({pred_unknown / total_injection * 100:.2f}%)\n")
            f.write(f"â†’ å…¶ä»–å·²çŸ¥æ”»å‡»: {pred_known_attack} ({pred_known_attack / total_injection * 100:.2f}%)\n")
            if known_attack_details:
                f.write(f"è¯¦ç»†åˆ†å¸ƒ: {known_attack_details}\n")
            f.write(f"Unknownæ£€æµ‹F1: {u_f1:.4f}\n")
            f.write(f"Unknownæ£€æµ‹ç²¾ç¡®ç‡: {u_precision:.4f}\n")
            f.write(f"Unknownæ£€æµ‹å¬å›ç‡: {u_recall:.4f}\n")
        else:
            f.write("æœªæ‰¾åˆ°injectionæ ·æœ¬\n")

    print(f"- è¯„ä¼°æŠ¥å‘Š: {report_path}")
    booster.save_model(os.path.join(RESULTS_DIR, "xgb_model.model"))
    print(f"- æ¨¡å‹æ–‡ä»¶: {os.path.join(RESULTS_DIR, 'xgb_model.model')}")


if __name__ == "__main__":
    main()