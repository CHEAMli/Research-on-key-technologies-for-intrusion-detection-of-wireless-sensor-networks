import os
import glob
import gc
from pathlib import Path
from datetime import datetime
from collections import Counter

import numpy as np
import pandas as pd
import pyarrow.parquet as pq

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import (
    confusion_matrix, f1_score, precision_recall_fscore_support,
    precision_score, recall_score
)

from sklearn.metrics import confusion_matrix

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

from model_doublecomparison import AnomalyDetectionModel

# ====================== åŸºæœ¬é…ç½® ======================
config = {
    "feature_dim": 16,        # ä¼šåœ¨åŠ è½½æ•°æ®åè‡ªåŠ¨æ›´æ–°
    "model_dim": 128,
    "tcn_layers": 2,
    "transformer_layers": 3,
    "nheads": 4,
    "dropout": 0.4,
    "max_len": 64,
    "epochs": 50,
    "batch_size": 64,
    "learning_rate": 5e-4,
    "weight_decay": 1e-4,
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "patience": 6,
    "gradient_clip": 1.0,
    "num_classes": 5,
    "window": 64,
    "stride": 64,
    "maj_threshold": 0.55,
    "aug_shift_max": 8,
    "aug_prob": 0.90,
}

# ====================== å›ºå®šéšæœºç§å­ ======================
def set_seed(seed: int = 42):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# ====================== è¯»å– parquet æ–‡ä»¶å¤¹ ======================
def load_parquet_folder(folder_path: str) -> pd.DataFrame:
    parquet_files = [
        f for f in glob.glob(os.path.join(folder_path, "*"))
        if f.endswith(".parquet") and not f.endswith(".parquet.crc")
    ]
    print(f"ğŸ“‚ åœ¨ {folder_path} ä¸­å‘ç° {len(parquet_files)} ä¸ªæœ‰æ•ˆ Parquet æ–‡ä»¶")
    dfs = []
    for file in parquet_files:
        try:
            table = pq.read_table(file)
            df = table.to_pandas()
            dfs.append(df)
            print(f"âœ… å·²è¯»å– {os.path.basename(file)}ï¼ŒåŒ…å« {len(df)} æ¡è®°å½•")
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡ {file}: {e}")
    if not dfs:
        raise RuntimeError(f"{folder_path} ä¸‹æ²¡æœ‰åˆæ³• parquet æ•°æ®")
    merged = pd.concat(dfs, ignore_index=True)
    print(f"âœ… åˆå¹¶åæ€»è®°å½•æ•°: {len(merged)}")
    return merged

# ====================== çª—å£æ•°æ®é›† ======================
class WindowsDataset(Dataset):
    def __init__(self, X: np.ndarray, y: np.ndarray, M: np.ndarray):
        self.X = torch.from_numpy(X).float()
        self.y = torch.from_numpy(y).long()
        self.M = torch.from_numpy(M).bool()
    def __len__(self):
        return self.X.shape[0]
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx], self.M[idx]

# ====================== A2 + B4 æ»‘çª—æ„é€  ======================
def make_windows_A2(
    df: pd.DataFrame,
    id_col: str,
    time_col: str,
    label_col: str,
    feature_cols,
    window: int,
    stride: int,
    maj_thr: float,
    aug_shift_max: int,
    aug_prob: float,
    idx_to_name,
    rare_ids=None,
):
    if rare_ids is None:
        rare_ids = set()
    df = df.sort_values([id_col, time_col])
    groups = df.groupby(id_col)

    X_list, y_list, M_list = [], [], []
    num_classes = len(idx_to_name)

    for sid, g in groups:
        feat = g[feature_cols].to_numpy(dtype=np.float32)
        lab = g[label_col].to_numpy(dtype=np.int64)
        L = len(g)
        if L == 0:
            continue
        s = 0
        while s < L:
            e = min(s + window, L)
            x_real = feat[s:e]
            y_real = lab[s:e]
            valid_len = e - s

            # padding + mask
            if valid_len < window:
                pad_len = window - valid_len
                pad = np.zeros((pad_len, feat.shape[1]), dtype=np.float32)
                x_win = np.concatenate([x_real, pad], axis=0)
                pad_mask = np.ones((window,), dtype=bool)
                pad_mask[:valid_len] = False
            else:
                x_win = x_real
                pad_mask = np.zeros((window,), dtype=bool)

            # A2 çª—å£æ ‡ç­¾ï¼šç¨€æœ‰ç±»ä¼˜å…ˆ + å¤šæ•°æŠ•ç¥¨
            if valid_len > 0:
                cnt = np.bincount(y_real.astype(np.int64), minlength=num_classes)
                present_rare = [(r, cnt[r]) for r in rare_ids if cnt[r] > 0]
                if present_rare:
                    # çª—å£é‡Œåªè¦æœ‰ç¨€æœ‰ç±»ï¼Œå°±é€‰å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç¨€æœ‰ç±»
                    y_win = max(present_rare, key=lambda z: z[1])[0]
                else:
                    ratio = cnt / (cnt.sum() + 1e-12)
                    if ratio.max() >= maj_thr:
                        y_win = int(np.argmax(cnt))
                    else:
                        # ä¸æ»¡è¶³é˜ˆå€¼ä¹Ÿä¸ä¸¢çª—å£ï¼Œé€€å›å‡ºç°æœ€å¤šçš„ç±»
                        y_win = int(np.argmax(cnt))
            else:
                y_win = 0

            X_list.append(x_win)
            y_list.append(y_win)
            M_list.append(pad_mask)
            s += stride

    X_win = np.stack(X_list, axis=0) if X_list else np.zeros((0, window, len(feature_cols)), np.float32)
    y_win = np.asarray(y_list, dtype=np.int64) if y_list else np.zeros((0,), np.int64)
    M_win = np.stack(M_list, axis=0) if M_list else np.zeros((0, window), np.bool_)

    counts = dict((int(c), int(v)) for c, v in Counter(y_win).items())
    print("[A2] window counts before B4:", counts)

    # B4ï¼šå¯¹ç¨€æœ‰ç±»æ—¶é—´å¹³ç§»å¢å¼ºï¼ˆåªåœ¨è®­ç»ƒé›†ï¼‰
    if y_win.size > 0 and aug_shift_max > 0 and aug_prob > 0:
        median_cnt = int(np.median(list(counts.values()))) if counts else 0
        rare_classes = {c for c, v in counts.items() if c in (rare_ids or set()) and v < median_cnt}
        if rare_classes:
            X_aug, y_aug, M_aug = [], [], []
            rng = np.random.default_rng()
            for x, y, m in zip(X_list, y_list, M_list):
                if y in rare_classes and rng.random() < aug_prob:
                    valid_idx = np.where(m == False)[0]
                    if valid_idx.size == 0:
                        continue
                    s0, e0 = valid_idx[0], valid_idx[-1] + 1
                    k = int(rng.integers(1, aug_shift_max + 1))
                    seg = x[s0:e0].copy()
                    seg = np.roll(seg, shift=k, axis=0)
                    x2 = x.copy()
                    x2[s0:e0] = seg
                    X_aug.append(x2)
                    y_aug.append(y)
                    M_aug.append(m.copy())
            if X_aug:
                print(f"[B4] augmented {len(X_aug)} windows for rare classes {sorted(rare_classes)}")
                X_win = np.concatenate([X_win, np.stack(X_aug, axis=0)], axis=0)
                y_win = np.concatenate([y_win, np.asarray(y_aug, dtype=np.int64)], axis=0)
                M_win = np.concatenate([M_win, np.stack(M_aug, axis=0)], axis=0)

    print("[A2/B4] final windows:", X_win.shape, y_win.shape, M_win.shape)
    if y_win.size:
        counts2 = dict((int(c), int(v)) for c, v in Counter(y_win).items())
        print("[A2/B4] window counts after B4:", counts2)
    return X_win, y_win, M_win

def infer_probs(model, loader, device):
    model.eval()
    all_probs = []
    all_y = []
    with torch.no_grad():
        for x, y, m in loader:
            x = x.to(device)
            m = m.to(device)
            logits = model(x, pad_mask=m)
            probs = torch.softmax(logits, dim=1).cpu().numpy()
            all_probs.append(probs)
            all_y.append(y.numpy())
    return np.concatenate(all_probs, axis=0), np.concatenate(all_y, axis=0)

def infer_logits(model, loader, device):
    """Return raw logits (no softmax) and labels from a dataloader."""
    model.eval()
    all_logits = []
    all_y = []
    with torch.no_grad():
        for x, y, m in loader:
            x = x.to(device)
            m = m.to(device)
            logits = model(x, pad_mask=m)
            all_logits.append(logits.cpu().numpy())
            all_y.append(y.numpy())
    return np.concatenate(all_logits, axis=0), np.concatenate(all_y, axis=0)

def infer_logits_feats(model, loader, device):
    model.eval()
    all_logits, all_feats, all_y = [], [], []
    with torch.no_grad():
        for x, y, m in loader:
            x = x.to(device)
            m = m.to(device)
            logits, feats = model(x, pad_mask=m, return_features=True)
            all_logits.append(logits.cpu().numpy())
            all_feats.append(feats.cpu().numpy())
            all_y.append(y.numpy())
    return (
        np.concatenate(all_logits, axis=0),
        np.concatenate(all_feats, axis=0),
        np.concatenate(all_y, axis=0),
    )

def softmax_np(z):
    z = z - np.max(z, axis=1, keepdims=True)
    e = np.exp(z)
    return e / (np.sum(e, axis=1, keepdims=True) + 1e-12)


def energy_score(logits: np.ndarray, T: float = 1.0) -> np.ndarray:
    """Energy score: E(x) = -T * logsumexp(logits/T). Higher => more OOD/unknown."""
    x = logits / T
    m = np.max(x, axis=1, keepdims=True)
    lse = m.squeeze(1) + np.log(np.sum(np.exp(x - m), axis=1))
    return -T * lse

# ====================== ä¸»è®­ç»ƒæµç¨‹ ======================
def main():
    save_dir = Path("saved_models")
    save_dir.mkdir(exist_ok=True)
    start_time = datetime.now()
    run_tag = start_time.strftime("%Y%m%d-%H%M%S")
    log_path = save_dir / "experiments_log.txt"

    def append_log(text: str):
        with log_path.open("a", encoding="utf-8") as f:
            f.write(text + "\n")

    append_log("")
    append_log(f"========== EXPERIMENT {run_tag} ==========")
    append_log(f"Started    : {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

    # ------------ 1. åŠ è½½åŸå§‹è®­ç»ƒ/æµ‹è¯•æ•°æ® ------------
    print("ğŸš€ å¼€å§‹åŠ è½½ WSN-DS æ•°æ®é›†...")
    train_df = load_parquet_folder(r"C:\Users\yeqing\PycharmProjects\pythonProject\WSN-DS-main\newtrain.parquet")
    test_df = load_parquet_folder(r"C:\Users\yeqing\PycharmProjects\pythonProject\WSN-DS-main\test.parquet")

    print("è®­ç»ƒé›†æ”»å‡»ç±»å‹åˆ†å¸ƒï¼š\n", train_df["Attack_type"].value_counts())
    print("æµ‹è¯•é›†æ”»å‡»ç±»å‹åˆ†å¸ƒï¼š\n", test_df["Attack_type"].value_counts())

    ID_COL = "id"
    TIME_COL = "Time"
    LABEL_COL = "Attack_type"

    UNKNOWN_FULL = 2  # Grayhole åœ¨ä½ çš„æ•°æ®é‡Œå°±æ˜¯ 2
    ALLOW_KNOWN_REJECT = 0.05  # å·²çŸ¥ç±»å…è®¸è¢«æ‹’è¯†çš„æ¯”ä¾‹ï¼ˆç”¨æ¥å®š tau_unknownï¼‰
    NORMAL_FPR = 0.05  # æ­£å¸¸æµé‡è¯¯æŠ¥ç‡ï¼ˆç”¨æ¥å®š tau_attackï¼‰
    SAFE_NORMAL_FPR = 0.01  # Safe-Normal Gateï¼šæ­£å¸¸è¢«åˆ¤ä¸ºâ€œéå®‰å…¨æ­£å¸¸â€çš„æ¯”ä¾‹ï¼ˆè¶Šå¤§è¶Šæ¿€è¿›ï¼‰

    PULL_NORM_FPR = 0.01  # åªå…è®¸ 1% çš„â€œçœŸå®æ­£å¸¸â€è¢«æˆ‘ä»¬ä» Normal æ‹½èµ°ï¼ˆæ§åˆ¶äºŒåˆ†ç±»ä¸å´©ï¼‰
    REJECT_ATK_FPR = 0.02  # åªå…è®¸ 2% çš„â€œçœŸå®å·²çŸ¥æ”»å‡»â€è¢«æ‹’è¯†æˆ Unknownï¼ˆæ§åˆ¶ known-only ä¸å´©ï¼‰

    SAFE_NORMAL_MD_FPR = 0.03  # å»ºè®® 0.02~0.05ï¼Œè¶Šå¤§è¶Šâ€œæ›´ä¸è®©è¿›Normalâ€
    COV_EPS = 1e-3  # åæ–¹å·®æ­£åˆ™ï¼Œé˜²æ­¢å¥‡å¼‚

    feature_cols = [c for c in train_df.columns if c not in [ID_COL, TIME_COL, LABEL_COL]]

    # ------------ 2. æ ‡å‡†åŒ– + å¡«å……ç¼ºå¤± ------------
    scaler = StandardScaler()
    train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])
    test_df[feature_cols] = scaler.transform(test_df[feature_cols])

    print("ğŸ”§ æ•°æ®é¢„å¤„ç†...")
    train_df = train_df.ffill().bfill()
    test_df = test_df.ffill().bfill()

    # ------------ 3. å·²çŸ¥ç±»æ˜ å°„ï¼ˆfull -> knownï¼‰ï¼ŒUnknown ä¸å‚ä¸è®­ç»ƒ ------------
    # train_df æ¥è‡ª newtrain.parquetï¼ˆå·²ç»æ²¡æœ‰ 2 äº†ï¼‰
    known_full = sorted([int(x) for x in train_df[LABEL_COL].unique().tolist()])
    assert UNKNOWN_FULL not in known_full, f"newtrain é‡Œå±…ç„¶è¿˜æœ‰ Unknown={UNKNOWN_FULL}ï¼Œå…ˆæ£€æŸ¥æ•°æ®é›†ï¼"

    full2known = {full: i for i, full in enumerate(known_full)}
    known2full = {i: full for full, i in full2known.items()}

    K = len(known_full)  # è¿™é‡Œåº”ä¸º 4
    config["num_classes"] = K
    print("[Known full labels]", known_full, "=> K =", K, "| Unknown(full) =", UNKNOWN_FULL)
    print("[Mapping full2known]", full2known)

    # è®­ç»ƒç”¨ known æ ‡ç­¾
    train_df[LABEL_COL] = train_df[LABEL_COL].map(full2known).astype(int)

    # test ä¿ç•™ full æ ‡ç­¾ï¼ˆç”¨äºåç»­ unknown è¯„ä¼°ï¼‰ï¼Œä¸è¦è¦†ç›–
    test_df["Attack_full"] = test_df[LABEL_COL].astype(int)

    # rare_idsï¼šå¯¹è®­ç»ƒç©ºé—´è€Œè¨€ï¼Œé0 éƒ½æ˜¯æ”»å‡»
    rare_ids = {c for c in range(K) if c != 0}
    idx_to_name_known = [str(known2full[i]) for i in range(K)]
    print("[Classes(Known)]", K, "=>", idx_to_name_known, "| rare_ids:", rare_ids)

    # ------------ 4. æŒ‰ A2+B4 æ„é€ è®­ç»ƒ/éªŒè¯çª—å£ ------------
    W = config["window"]
    ST = config["stride"]
    TH = config["maj_threshold"]

    X_all, y_all, M_all = make_windows_A2(
        train_df, ID_COL, TIME_COL, LABEL_COL, feature_cols,
        W, ST, TH,
        config["aug_shift_max"], config["aug_prob"],
        idx_to_name_known, rare_ids=rare_ids,
    )
    X_tr, X_va, y_tr, y_va, M_tr, M_va = train_test_split(
        X_all, y_all, M_all,
        test_size=0.2,
        random_state=42,
        stratify=y_all
    )

    test_df_full = test_df.copy()
    test_df_full[LABEL_COL] = test_df_full["Attack_full"].astype(int)

    idx_to_name_full = [str(i) for i in range(5)]  # 0..4
    rare_ids_full = {1, 2, 3, 4}

    X_te, y_te_full, M_te = make_windows_A2(
        test_df_full, ID_COL, TIME_COL, LABEL_COL, feature_cols,
        W, ST, TH,
        0, 0.0,
        idx_to_name_full, rare_ids=rare_ids_full,
    )

    print("[Final window counts]", dict(Counter(y_tr.tolist())))
    config["feature_dim"] = X_tr.shape[2]
    print(f"ç‰¹å¾ç»´åº¦æ›´æ–°ä¸º: {config['feature_dim']}")

    train_dataset = WindowsDataset(X_tr, y_tr, M_tr)
    val_dataset = WindowsDataset(X_va, y_va, M_va)
    train_loader = DataLoader(train_dataset, batch_size=config["batch_size"], shuffle=True, num_workers=0, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=config["batch_size"], shuffle=False, num_workers=0, pin_memory=True)

    gc.collect()

    # ------------ 5. æ„å»ºæ¨¡å‹ ------------
    model = AnomalyDetectionModel(
        feature_dim=config["feature_dim"],
        model_dim=config["model_dim"],
        num_classes=config["num_classes"],
        tcn_layers=config["tcn_layers"],
        transformer_layers=config["transformer_layers"],
        nheads=config["nheads"],
        dropout=config["dropout"],
        max_len=config["max_len"],
    ).to(config["device"])

    print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    # ------------ 6. ä¼˜åŒ–å™¨ + è°ƒåº¦å™¨ + æŸå¤±å‡½æ•° ------------
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=config["learning_rate"],
        weight_decay=config["weight_decay"],
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config["epochs"],
        eta_min=config["learning_rate"] * 0.1,
    )

    # â€”â€”å¤šåˆ†ç±» CrossEntropy + ç±»åˆ«æƒé‡ï¼ˆå°±æ˜¯ä½  best é‚£ç‰ˆçš„æ ¸å¿ƒï¼‰â€”â€”
    class_weights = compute_class_weight(
        class_weight="balanced",
        classes=np.arange(K),
        y=y_tr,
    ).astype(np.float32)

    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32, device=config["device"])
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)
    print("[class_weights]", {i: float(w) for i, w in enumerate(class_weights)})

    # ------------ 7. è®­ç»ƒå¾ªç¯ ------------
    best_f1 = 0.0
    best_epoch = -1
    no_improve = 0
    all_train_losses, all_val_losses = [], []
    all_train_f1s, all_val_f1s = [], []

    labels_all = np.arange(K)

    for epoch in range(config["epochs"]):
        # === Train ===
        model.train()
        train_loss = 0.0
        train_preds, train_targets = [], []

        from tqdm.auto import tqdm
        for inputs, labels, pad_mask in tqdm(
            train_loader,
            desc=f"[Train] {epoch+1}/{config['epochs']}",
            leave=False,
            dynamic_ncols=True,
        ):
            inputs = inputs.to(config["device"])
            labels = labels.to(config["device"])
            pad_mask = pad_mask.to(config["device"])

            optimizer.zero_grad()
            outputs = model(inputs, pad_mask=pad_mask)
            loss = criterion(outputs, labels)
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=config["gradient_clip"])
            optimizer.step()

            train_loss += loss.item()
            preds = outputs.argmax(dim=1)
            train_preds.extend(preds.detach().cpu().numpy().tolist())
            train_targets.extend(labels.detach().cpu().numpy().tolist())

        train_avg_loss = train_loss / len(train_loader)
        train_f1 = f1_score(train_targets, train_preds, average="macro", labels=labels_all, zero_division=0)
        print(f"Epoch {epoch+1} - è®­ç»ƒ - æŸå¤±: {train_avg_loss:.4f}, F1: {train_f1:.4f}")
        print("è®­ç»ƒé›†æ··æ·†çŸ©é˜µ:\n", confusion_matrix(train_targets, train_preds))
        all_train_losses.append(train_avg_loss)
        all_train_f1s.append(train_f1)

        # === Val ===
        model.eval()
        val_loss = 0.0
        val_preds, val_targets = [], []
        with torch.no_grad():
            for inputs, labels, pad_mask in val_loader:
                inputs = inputs.to(config["device"])
                labels = labels.to(config["device"])
                pad_mask = pad_mask.to(config["device"])

                outputs = model(inputs, pad_mask=pad_mask)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

                preds = outputs.argmax(dim=1)
                val_preds.extend(preds.detach().cpu().numpy().tolist())
                val_targets.extend(labels.detach().cpu().numpy().tolist())

        val_avg_loss = val_loss / len(val_loader)
        val_f1 = f1_score(val_targets, val_preds, average="macro", labels=labels_all, zero_division=0)
        print(f"Epoch {epoch+1}/{config['epochs']} - éªŒè¯ - æŸå¤±: {val_avg_loss:.4f}, F1: {val_f1:.4f}")
        print("éªŒè¯é›†æ··æ·†çŸ©é˜µ:\n", confusion_matrix(val_targets, val_preds))

        labels_all = list(range(K))

        prec, rec, f1c, sup = precision_recall_fscore_support(
            val_targets,
            val_preds,
            labels=labels_all,
            average=None,
            zero_division=0
        )

        print("\n[Per-Class on VAL]")
        for i in range(K):
            name = idx_to_name_known[i]
            print(f"{name:<5} P={prec[i]:.3f} R={rec[i]:.3f} F1={f1c[i]:.3f} N={int(sup[i])}")

        scheduler.step()
        all_val_losses.append(val_avg_loss)
        all_val_f1s.append(val_f1)

        # æ—©åœ + ä¿å­˜æœ€ä¼˜
        if val_f1 > best_f1:
            best_f1 = val_f1
            best_epoch = epoch + 1
            no_improve = 0
            torch.save(
                {
                    "epoch": epoch + 1,
                    "model_state_dict": model.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "scheduler_state_dict": scheduler.state_dict(),
                    "val_f1": val_f1,
                    "config": config,
                },
                save_dir / "best_model.pth",
            )
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒF1: {best_f1:.4f}")
        else:
            no_improve += 1
            print(f"â¸ æ¨¡å‹æ€§èƒ½æœªæå‡ ({no_improve}/{config['patience']})")
            if no_improve >= config["patience"]:
                print(f"ğŸ›‘ æå‰åœæ­¢ï¼š{config['patience']} ä¸ª epoch æœªæ”¹å–„")
                break

    # ä¿å­˜è®­ç»ƒæ›²çº¿
    np.savez(
        save_dir / "training_history.npz",
        train_losses=np.array(all_train_losses),
        val_losses=np.array(all_val_losses),
        train_f1s=np.array(all_train_f1s),
        val_f1s=np.array(all_val_f1s),
    )

    # ------------ 8. Unknown/æ‹’è¯†æœºåˆ¶ï¼šå…ˆåœ¨ VAL ä¸Šå®šé˜ˆå€¼ï¼Œå†åœ¨ TEST ä¸Šè¯„ä¼°ï¼ˆåŠ å…¥ Safe-Normal Gateï¼‰ ------------

    # é‡æ–°åŠ è½½ best_model

    ckpt = torch.load(save_dir / "best_model.pth", map_location=config["device"])

    model.load_state_dict(ckpt["model_state_dict"])

    normal_known_id = int(full2known.get(0, 0))

    # =======================
    # 1) VALï¼šæ ‡å®šé˜ˆå€¼ï¼ˆå·²çŸ¥ç±»ï¼‰
    # =======================

    val_logits, val_feats, val_y = infer_logits_feats(model, val_loader, config["device"])
    val_probs = softmax_np(val_logits)

    E_val = energy_score(val_logits, T=1.0)

    pnormal_val = val_probs[:, normal_known_id]

    # ---------- Feature-prototype for Safe-Normal Gate ----------
    val_pred = val_probs.argmax(axis=1)
    mask_good_norm = (val_y == normal_known_id) & (val_pred == normal_known_id)  # åªç”¨â€œé¢„æµ‹ä¹Ÿä¸ºæ­£å¸¸â€çš„å¹²å‡€æ­£å¸¸æ¥å®šé˜ˆå€¼

    # class prototypes in feature space
    proto_feat = []
    for k in range(K):
        mk = (val_y == k)
        if mk.sum() == 0:
            proto_feat.append(np.zeros((val_feats.shape[1],), dtype=np.float32))
        else:
            proto_feat.append(val_feats[mk].mean(axis=0))
    proto_feat = np.stack(proto_feat, axis=0)  # [K, D]
    proto_feat_norm = proto_feat / (np.linalg.norm(proto_feat, axis=1, keepdims=True) + 1e-12)

    val_feats_norm = val_feats / (np.linalg.norm(val_feats, axis=1, keepdims=True) + 1e-12)
    sim_feat_val = val_feats_norm @ proto_feat_norm.T
    sim0_feat_val = sim_feat_val[:, normal_known_id]
    sim_other_feat_val = np.max(np.delete(sim_feat_val, normal_known_id, axis=1), axis=1)
    margin_feat_val = sim0_feat_val - sim_other_feat_val

    # ========= (NEW) Mahalanobis Normal-Gate in feature space =========
    def _fit_gaussian_inv(feats: np.ndarray, eps: float = COV_EPS):
        mu = feats.mean(axis=0)
        cov = np.cov(feats, rowvar=False)
        cov = cov + np.eye(cov.shape[0]) * eps
        inv = np.linalg.inv(cov)
        return mu, inv

    def _mahalanobis(feats: np.ndarray, mu: np.ndarray, inv_cov: np.ndarray):
        diff = feats - mu
        return np.einsum("ni,ij,nj->n", diff, inv_cov, diff)

    # ç”¨â€œå¹²å‡€æ­£å¸¸â€(val ä¸­è¢«æ¨¡å‹åˆ¤å¯¹ Normal çš„æ ·æœ¬) æ‹Ÿåˆ Normal åˆ†å¸ƒ
    feats_norm_clean = val_feats[mask_good_norm]
    mu_norm, inv_cov_norm = _fit_gaussian_inv(feats_norm_clean, eps=COV_EPS)
    md_val_clean = _mahalanobis(feats_norm_clean, mu_norm, inv_cov_norm)
    tau_md_pull = float(np.quantile(md_val_clean, 1.0 - SAFE_NORMAL_MD_FPR))  # md è¶Šå¤§è¶Šä¸åƒæ­£å¸¸

    # Safe-Normal thresholds: å…è®¸ SAFE_NORMAL_FPR æ¯”ä¾‹çš„â€œæ­£å¸¸â€è¢«æ‹’
    tau_sim0_feat_safe = float(np.quantile(sim0_feat_val[mask_good_norm], SAFE_NORMAL_FPR))
    tau_margin_feat_safe = float(np.quantile(margin_feat_val[mask_good_norm], SAFE_NORMAL_FPR))

    # åŒæ—¶æŠŠä½ åŸæ¥çš„ p0/E é˜ˆå€¼æ–¹å‘æ”¹æˆâ€œæ‹’æ‰å°¾éƒ¨â€ï¼Œåˆ«ç”¨ 99% é‚£ç§å†™æ³•
    tau_p0_safe = float(np.quantile(pnormal_val[mask_good_norm], SAFE_NORMAL_FPR))  # p0 çš„ä½å°¾
    tau_E_safe = float(np.quantile(E_val[mask_good_norm], 1.0 - SAFE_NORMAL_FPR))  # E çš„é«˜å°¾ï¼ˆEè¶Šå¤§è¶Šä¸åƒæ­£å¸¸ï¼‰

    # Unknown(Energy) é˜ˆå€¼ï¼šå…è®¸ ALLOW_KNOWN_REJECT çš„å·²çŸ¥æ ·æœ¬è¢«æ‹’è¯†

    tau_energy = float(np.quantile(E_val, 1.0 - ALLOW_KNOWN_REJECT))

    # Attack é˜ˆå€¼ï¼šå…è®¸ NORMAL_FPR çš„æ­£å¸¸æ ·æœ¬è¢«è¯¯æŠ¥ä¸ºæ”»å‡»

    s_attack_val = 1.0 - pnormal_val

    mask_norm_val = (val_y == normal_known_id)

    tau_attack = float(np.quantile(s_attack_val[mask_norm_val], 1.0 - NORMAL_FPR))

    # ---- (NEW) Logits-prototype features for safer Normal gate ----
    # Build class prototypes (mean logits) on VAL (known classes only).
    proto_logits = []
    for k in range(K):
        mk = (val_y == k)
        if mk.sum() == 0:
            proto_logits.append(np.zeros((val_logits.shape[1],), dtype=np.float32))
        else:
            proto_logits.append(val_logits[mk].mean(axis=0))
    proto_logits = np.stack(proto_logits, axis=0)  # [K, K]
    proto_norm = proto_logits / (np.linalg.norm(proto_logits, axis=1, keepdims=True) + 1e-12)

    val_logits_norm = val_logits / (np.linalg.norm(val_logits, axis=1, keepdims=True) + 1e-12)
    sim_val = val_logits_norm @ proto_norm.T  # [N, K] cosine(logits, proto)
    sim0_val = sim_val[:, normal_known_id]
    sim_other_max_val = np.max(np.delete(sim_val, normal_known_id, axis=1), axis=1)
    margin0_val = sim0_val - sim_other_max_val

    # Safe-Normal extra thresholds from VAL-Normal (lower tail):
    tau_sim0_safe = float(np.quantile(sim0_val[mask_norm_val], SAFE_NORMAL_FPR))
    tau_margin0_safe = float(np.quantile(margin0_val[mask_norm_val], SAFE_NORMAL_FPR))

    # ====== (NEW) ä»…ç”¨äºâ€œå åŠ  Unknownâ€çš„é˜ˆå€¼ï¼šä¿è¯ä¸ä¼¤ä½ åŸæœ¬æœ€ä¼˜åˆ†ç±» ======

    # 1) Normal æ‹‰å‡ºé˜ˆå€¼ï¼šåªæ‹‰å‡º val-normal ä¸­æœ€å¯ç–‘çš„ PULL_NORM_FPR éƒ¨åˆ†
    p2_val = np.partition(val_probs, -2, axis=1)[:, -2]  # ç¬¬äºŒå¤§æ¦‚ç‡
    margin0_val_soft = pnormal_val - p2_val  # Normal ç½®ä¿¡åº¦é—´éš”ï¼ˆè¶Šå°è¶Šä¸å¯ä¿¡ï¼‰
    tau_margin0_pull = float(np.quantile(margin0_val_soft[mask_norm_val], PULL_NORM_FPR))

    tau_E_norm_pull = float(np.quantile(E_val[mask_norm_val], 1.0 - PULL_NORM_FPR))  # æ­£å¸¸èƒ½é‡é«˜å°¾ï¼šå¯ç–‘
    tau_attack_pull = float(np.quantile(s_attack_val[mask_norm_val], 1.0 - PULL_NORM_FPR))  # (1-p0)é«˜å°¾ï¼šå¯ç–‘

    # 2) Attack æ‹’è¯†é˜ˆå€¼ï¼šåªæ‹’è¯† val-attack ä¸­æœ€ä¸å¯ä¿¡çš„ REJECT_ATK_FPR éƒ¨åˆ†
    mask_atk_val = ~mask_norm_val
    probs_non0_val = val_probs.copy()
    probs_non0_val[:, normal_known_id] = -1.0
    atk_top1_val = probs_non0_val.max(axis=1)
    atk_top2_val = np.partition(probs_non0_val, -2, axis=1)[:, -2]
    atk_gap_val = atk_top1_val - atk_top2_val

    tau_atk_conf_rej = float(np.quantile(atk_top1_val[mask_atk_val], REJECT_ATK_FPR))  # æ”»å‡» top1 ä½å°¾
    tau_atk_gap_rej = float(np.quantile(atk_gap_val[mask_atk_val], REJECT_ATK_FPR))  # æ”»å‡» gap ä½å°¾
    tau_E_atk_rej = float(np.quantile(E_val[mask_atk_val], 1.0 - REJECT_ATK_FPR))  # æ”»å‡»èƒ½é‡é«˜å°¾

    print(
        f"[Overlay THR] "
        f"tau_attack_pull={tau_attack_pull:.4f} | "
        f"tau_margin0_pull={tau_margin0_pull:.4f} | "
        f"tau_E_norm_pull={tau_E_norm_pull:.4f} | "
        f"tau_atk_conf_rej={tau_atk_conf_rej:.4f} | "
        f"tau_atk_gap_rej={tau_atk_gap_rej:.4f} | "
        f"tau_E_atk_rej={tau_E_atk_rej:.4f}"
    )

    print(
        f"[Thresholds] "
        f"tau_energy(E>{tau_energy:.4f}) | "
        f"tau_attack(1-p0>{tau_attack:.4f}) | "
        f"tau_p0_safe(p0>={tau_p0_safe:.4f}) | "
        f"tau_E_safe(E<={tau_E_safe:.4f})"
    )
    # =======================
    # 2) TESTï¼šæ¨ç† + è¯„ä¼°
    # =======================

    test_dataset = WindowsDataset(X_te, y_te_full, M_te)  # y_te_full æ˜¯ full æ ‡ç­¾ï¼ˆå« Unknownï¼‰
    test_loader = DataLoader(test_dataset, batch_size=config["batch_size"], shuffle=False)

    te_logits, te_feats, te_y_full = infer_logits_feats(model, test_loader, config["device"])
    te_probs_known = softmax_np(te_logits)

    E_te = energy_score(te_logits, T=1.0)

    pred_known = te_probs_known.argmax(axis=1)

    pnormal_te = te_probs_known[:, normal_known_id]

    s_attack_te = 1.0 - pnormal_te

    # =======================
    # 2) TESTï¼šæ¨ç† + å åŠ  Unknownï¼ˆä¸ç ´åä½ åŸæœ¬æœ€ä¼˜åˆ†ç±»ï¼‰
    # =======================

    # å…ˆåšâ€œåŸæœ¬æœ€ä¼˜â€çš„ baseline å¤šåˆ†ç±»ï¼ˆç»ä¸ force non0ï¼‰
    pred_known = te_probs_known.argmax(axis=1)
    pred_full = np.array([known2full[int(k)] for k in pred_known], dtype=int)

    # è®¡ç®—åŸºç¡€åˆ†æ•°
    pnormal_te = te_probs_known[:, normal_known_id]
    s_attack_te = 1.0 - pnormal_te

    te_feats_norm = te_feats / (np.linalg.norm(te_feats, axis=1, keepdims=True) + 1e-12)
    md_te = _mahalanobis(te_feats, mu_norm, inv_cov_norm)

    sim_feat_te = te_feats_norm @ proto_feat_norm.T
    sim0_feat_te = sim_feat_te[:, normal_known_id]
    sim_other_feat_te = np.max(np.delete(sim_feat_te, normal_known_id, axis=1), axis=1)
    margin_feat_te = sim0_feat_te - sim_other_feat_te

    # Normal ç½®ä¿¡åº¦é—´éš”ï¼ˆsoftmax marginï¼‰
    p2_te = np.partition(te_probs_known, -2, axis=1)[:, -2]
    margin0_te_soft = pnormal_te - p2_te

    # logits-prototype çš„ sim/marginï¼ˆä½ åŸæ¥ç®—è¿‡ï¼Œå°±ä¿ç•™è¿™æ®µï¼‰
    te_logits_norm = te_logits / (np.linalg.norm(te_logits, axis=1, keepdims=True) + 1e-12)
    sim_te = te_logits_norm @ proto_norm.T
    sim0_te = sim_te[:, normal_known_id]
    sim_other_max_te = np.max(np.delete(sim_te, normal_known_id, axis=1), axis=1)
    margin0_te_proto = sim0_te - sim_other_max_te

    # Attack ä¾§ top1/top2 gapï¼ˆsoftmaxï¼‰
    probs_non0_te = te_probs_known.copy()
    probs_non0_te[:, normal_known_id] = -1.0
    atk_top1_te = probs_non0_te.max(axis=1)
    atk_top2_te = np.partition(probs_non0_te, -2, axis=1)[:, -2]
    atk_gap_te = atk_top1_te - atk_top2_te

    # ---------- å åŠ  Unknownï¼šä¸¤ç±»è§¦å‘ ----------
    # A) æœ€é«˜ä¼˜å…ˆçº§ï¼šæŠŠâ€œé¢„æµ‹ä¸º Normal çš„å¯ç–‘æ ·æœ¬â€ç›´æ¥æ”¹æˆ Unknownï¼ˆä¼˜å…ˆé™ä½ Grayhole->Normalï¼‰
    mask_pred_norm = (pred_full == 0)
    mask_pull_from_normal = mask_pred_norm & (
            (s_attack_te > tau_attack_pull) |
            (margin0_te_soft < tau_margin0_pull) |
            (E_te > tau_E_norm_pull) |
            (md_te > tau_md_pull) |  # (NEW) Mahalanobisï¼šé«˜ç½®ä¿¡åº¦ä½†ç‰¹å¾åˆ†å¸ƒå¼‚å¸¸çš„â€œä¼ªæ­£å¸¸â€
            (margin0_te_proto < tau_margin0_safe) |
            (sim0_te < tau_sim0_safe) |
            # ===== æ–°å¢ï¼šç”¨â€œç‰¹å¾ç©ºé—´â€çš„åŸå‹ç›¸ä¼¼åº¦æ¥æ‹‰ Unknownï¼ˆä¸“é—¨æ‰“ Grayhole->Normalï¼‰=====
            (sim0_feat_te < tau_sim0_feat_safe) |
            (margin_feat_te < tau_margin_feat_safe)
    )

    # B) æ¬¡ä¼˜å…ˆçº§ï¼šæŠŠâ€œé¢„æµ‹ä¸ºæ”»å‡»ä½†å¾ˆä¸å¯ä¿¡â€çš„æ ·æœ¬æ”¹æˆ Unknownï¼ˆé™ä½ Grayhole->Other Attackï¼Œæå‡ Unknownï¼‰
    mask_pred_atk = (pred_full != 0)
    mask_reject_attack = mask_pred_atk & (
            (atk_top1_te < tau_atk_conf_rej) |
            (atk_gap_te < tau_atk_gap_rej) |
            (E_te > tau_E_atk_rej)
    )

    pred_is_unknown_final = mask_pull_from_normal | mask_reject_attack
    pred_full[pred_is_unknown_final] = UNKNOWN_FULL

    # =======================
    # 3) æŒ‡æ ‡ä¸ç»Ÿè®¡
    # =======================
    gt_is_unknown = (te_y_full == UNKNOWN_FULL)
    gt_is_attack = (te_y_full != 0)

    pred_is_attack_final = (pred_full != 0)

    p_attack = precision_score(gt_is_attack.astype(int), pred_is_attack_final.astype(int), zero_division=0)

    r_attack = recall_score(gt_is_attack.astype(int), pred_is_attack_final.astype(int), zero_division=0)

    f1_attack = f1_score(gt_is_attack.astype(int), pred_is_attack_final.astype(int), zero_division=0)

    auc_attack = roc_auc_score(gt_is_attack.astype(int), s_attack_te)

    print(f"[Attack vs Normal] P={p_attack:.4f} R={r_attack:.4f} F1={f1_attack:.4f} AUROC={auc_attack:.4f}")

    # --- Attack vs Normal 2x2 confusion matrix ---
    gt_ab = gt_is_attack.astype(int)  # 0=Normal, 1=Attack
    pd_ab = pred_is_attack_final.astype(int)

    cm = confusion_matrix(gt_ab, pd_ab, labels=[0, 1])
    tn, fp, fn, tp = cm.ravel()

    print(f"[Attack vs Normal CM] [[TN FP],[FN TP]] = {cm.tolist()}")
    print(f"[Attack vs Normal ERR] Normal->Attack(FP)={fp} | Attack->Normal(FN)={fn}")
    print(f"[Attack vs Normal RATE] FPR={fp / (fp + tn + 1e-12):.4f} | FNR={fn / (fn + tp + 1e-12):.4f}")

    p_u = precision_score(gt_is_unknown.astype(int), pred_is_unknown_final.astype(int), zero_division=0)

    r_u = recall_score(gt_is_unknown.astype(int), pred_is_unknown_final.astype(int), zero_division=0)

    f1_u = f1_score(gt_is_unknown.astype(int), pred_is_unknown_final.astype(int), zero_division=0)

    auc_u_energy = roc_auc_score(gt_is_unknown.astype(int), E_te)

    print(f"[Unknown] P={p_u:.4f} R={r_u:.4f} F1={f1_u:.4f} AUROC(E)={auc_u_energy:.4f}")

    # Known-only å®F1ï¼šçœŸå®å·²çŸ¥ & æœªæ‹’è¯†
    mask_known_eval = (~gt_is_unknown) & (~pred_is_unknown_final)

    if mask_known_eval.sum() > 0:
        yk = np.array([full2known.get(int(y), -1) for y in te_y_full[mask_known_eval]], dtype=int)
        pk = np.array([full2known.get(int(y), -1) for y in pred_full[mask_known_eval]], dtype=int)
        macro_f1_known = f1_score(yk, pk, average="macro", labels=list(range(K)), zero_division=0)
        coverage = float(mask_known_eval.mean())
        print(f"[Known-only] Macro-F1={macro_f1_known:.4f} | coverage={coverage:.4f}")

    # Grayhole å»å‘ï¼šUnknown / Normal / Other Attack
    mask_gh = (te_y_full == UNKNOWN_FULL)

    n_gh = int(mask_gh.sum())

    if n_gh > 0:
        gh_as_unknown = int((mask_gh & pred_is_unknown_final).sum())
        gh_as_normal  = int((mask_gh & (pred_full == 0)).sum())
        gh_as_other   = int((mask_gh & (pred_full != 0) & (pred_full != UNKNOWN_FULL)).sum())
        print("")
        print("[Grayhole Breakdown]")
        print(f"Total Grayhole windows      : {n_gh}")
        print(f"Pred as Unknown (reject)    : {gh_as_unknown}  ({gh_as_unknown/max(n_gh,1):.4f})")
        print(f"Pred as Normal (0)          : {gh_as_normal}   ({gh_as_normal/max(n_gh,1):.4f})")
        print(f"Pred as Other Attack (1/3/4): {gh_as_other} ({gh_as_other/max(n_gh,1):.4f})")
        vals, cnts = np.unique(pred_full[mask_gh], return_counts=True)
        print("[Grayhole predicted full label counts]", dict(zip(vals.tolist(), cnts.tolist())))

    print(f"ğŸ¯ è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯ F1: {best_f1:.4f}, epoch={best_epoch}")
if __name__ == "__main__":
    main()
